## User Guide on Pytorch Guide:

This library was created by keeping in mind usability and future use so its use is tailored for machine learning analysis of nuclear fuel as it depletes over time but tweaks and different models can be added simply by adding further pytorch modules. \
This library comes with a DNN module which is the module used to produce the results in our project reports.\
The training and testing of the model is controlled by a .yaml file which the user can edit, the current parameters and their values are as follow under each of the .yaml keywords.

# Configuration Guide

## Dataset Configuration

### `dataset`
- **`path_to_data`**: `string`
  - Path to the directory containing CSV data files
  
- **`fraction_of_data`**: `float` (0.0 to 1.0)
  - Fraction of the dataset to use for training/validation
  - `1.0` = use entire dataset, `0.5` = use 50% of data

- **`inputs`**: `dictionary`
  - Contains the columns to include as the inputs to the model, and their specific scaling
  - The possible scalers are:  MinMax, Standard, Robust, MaxAbs, Normalizer, Quantile, Power and None if no scaling should be applied. 
  ```yaml
    power_W_g: "MinMax"
    U238: "robust"
    ```

- **`targets`**: `dictionary`
  - Similalry to the inputs these columns will be included as the targets and outputs of the model. Notably the inputs are at time `t` while the targets are at time `t+1`. Hence you can have a column be both a target and an input, common in many time series predictions tasks.

- **`target_delta_conc`**: `boolean`
  - **Options**:
    - `True`: model predicts concentration difference for each step
    - `False`: model predicts absolute values

### Data Loaders

#### `train`
- **`batch_size`**: `integer`
  - Number of samples per training batch
  - Larger batches = more stable gradients, higher memory usage but worse generalization

#### `val`
- **`batch_size`**: `integer`
  - Number of samples per validation batch
  - Should be as large as possible on the available hardware as gradients are not computed

---

## Model Configuration

### `model`
- **`name`**: `string`
  - Model identifier used for saving results and logging

- **`layers`**: `list[integer]`
  - Architecture of hidden layers
  - Example: `[64, 64]` = two hidden layers with 64 neurons each
  - Example: `[128, 64, 32]` = three hidden layers with decreasing sizes

- **`dropout_probability`**: `float` (0.0 to 1.0)
  - Dropout rate for regularization
  - `0.0` = no dropout, `0.1` = 10% of neurons dropped

- **`activation`**: `string`
  - Activation function for hidden layers
  - **Options**: `"relu"`, `"tanh"`, `"sigmoid"`, `"leaky_relu"`, `"elu"`, `"gelu"`

- **`output_activation`**: `string`
  - Activation function for output layer
  - **Options**: `"none"`, `"sigmoid"`, `"tanh"`, `"softplus"`
  - Use `"none"` for regression tasks

- **`residual_connections`**: `boolean`
  - **Options**:
    - `True`: Enable skip connections between layers (only works if two adjacent layers match in the number of inputs and outputs)
    - `False`: Standard feedforward connections

---

## Training Configuration

### `train`
- **`loss`**: `string`
  - Loss function for optimization
  - **Options**: `"mse"` (Mean Squared Error), `"mae"` (Mean Absolute Error), `"huber"`, `"smooth_l1"`

- **`learning_rate`**: `float`
  - Initial learning rate for optimizer
  - Typical range: `1e-5` to `1e-2`

- **`weight_decay`**: `float`
  - L2 regularization penalty
  - `0.0` = no regularization, typical values: `1e-5` to `1e-3`

- **`num_epochs`**: `integer`
  - Maximum number of training epochs

- **`lr_scheduler_patience`**: `integer`
  - Number of epochs without improvement before reducing learning rate
  - Used with ReduceLROnPlateau scheduler

- **`early_stopping_patience`**: `integer`
  - Number of epochs without improvement before stopping training
  - Prevents overfitting

- **`dropout_probability`**: `float` (0.0 to 1.0)
  - Dropout rate during training (can override model dropout)

---

## Output Configuration

### `output`
- **`result_dir`**: `string`
  - Directory path for saving results, checkpoints, and logs
  - Supports variable interpolation: `${model.name}` inserts the model name

---

## Runtime Configuration

### `runtime`
- **`mode`**: `string`
  - Execution mode
  - **Options**:
    - `"train"`: Train from scratch
    - `"train_from_ckp"`: Resume training from checkpoint

- **`ckp_path`**: `string`
  - Path to checkpoint file for resuming training
  - Only used when `mode = "train_from_ckp"`

- **`device`**: `string`
  - Computation device
  - **Options**: `"cuda"` (GPU), `"cpu"`

- **`seed`**: `integer`
  - Random seed for reproducibility
  - Set to same value for deterministic splitting of the data, notably PyTorch weights are not seeded so slightly different results can occur

- **`drop_last`**: `boolean`
  - **Options**:
    - `True`: Drop incomplete final batch
    - `False`: Keep all data including incomplete batch

- **`num_workers`**: `integer`
  - Number of parallel workers for data loading
  - `0` = load in main process, `>0` = use multiprocessing
---